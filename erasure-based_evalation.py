import tensorflow.compat.v1 as tf
from utils.parser import parse_args
import os
from utils.dataset import DoubanMovie, Yelp, LastFM
from models.recommender import MFModule, BasePipeline
from utils.metrics import *
import pandas as pd
from utils.func import print_dict
import numpy as np
import random

cuda0 = torch.device('cuda:0')

def concate_fusion(a_embedding, b_embedding):
    return torch.mul(a_embedding.to(cuda0), b_embedding.to(cuda0))

def random_baseline(our_explanations,data,args):
    exps={}
    for user in our_explanations.index: # ensure same users/item for evaluation
        exps[user] = random.sample(data.candidate_attributes.attribute_id.tolist(),args.topK)
    exps = pd.DataFrame.from_dict(exps,orient='index')
    exps.to_csv(os.path.join(args_config.explanation_path,
                                       'random_attribute_{}.csv'.format(args_config.dataset,
                                                                                   )), index=False)
    #
def pop_baseline(our_explanations,data,args):



    u_att = data.candidate_attributes[:data.UvsU.shape[0]+data.UvsG.shape[0]]
    i_att = data.candidate_attributes[data.UvsU.shape[0]+data.UvsG.shape[0]:]
    
    u_att_sort = u_att.groupby(['attribute_id']).count().sort_values('node_id', ascending=False)['node_id']
    u_top_split = int(u_att_sort.shape[0] * 0.8)
    
    i_att_sort = i_att.groupby(['attribute_id']).count().sort_values('node_id', ascending=False)['node_id']
    i_top_split = int(i_att_sort.shape[0] * 0.8)
    
    top_u_att = np.array(u_att_sort[:u_top_split].index).tolist()
    top_i_att = np.array(i_att_sort[:i_top_split].index).tolist()
    
    
    u_pop_exps = {}
    i_pop_exps = {}
    for user in our_explanations.index:  # ensure same users/item for evaluation
        u_pop_exps[user] = random.sample(top_u_att, args.topK)
        i_pop_exps[user] = random.sample(top_i_att, args.topK)
    
    u_exps = pd.DataFrame.from_dict(u_pop_exps, orient='index')
    u_exps.to_csv(os.path.join(args_config.explanation_path,
                             'user_pop_attribute_{}.csv'.format(args_config.dataset,
                                                              )), index=False)
    
    i_exps = pd.DataFrame.from_dict(i_pop_exps, orient='index')
    i_exps.to_csv(os.path.join(args_config.explanation_path,
                             'item_pop_attribute_{}.csv'.format(args_config.dataset,
                                                                )), index=False)

def erasure_evaluation(rec_model, explanations, attribute_emb, data, erasure):
    """
    Each data point is generated by cumulatively removing
    top n features in the explanation lists provided by explanation methods.
    """

    user_att_index = [*range(0, data.n_friends + data.n_groups)]
    print('all user attribute num:',len(user_att_index))
    item_att_index = [*range(data.n_friends + data.n_groups, data.n_actors + data.n_directors + data.n_types)]
    print('all item attribute num:',len(item_att_index))
    user_att_index = set(user_att_index)
    item_att_index = set(item_att_index)

    erasure_set = explanations.iloc[:, :erasure].values.flatten()

    for i in range(0, len(erasure_set), 1024):
        print('erasure number for this batch:', i)
        erasure = set(erasure_set[:i])

        erased_user_att = user_att_index - erasure
        erased_item_att = item_att_index - erasure
        print('user attribute num after erasure:', len(erased_user_att),'item attribute num after erasure:',len(erased_item_att))

        user_atts = attribute_emb[list(erased_user_att), :]
        item_atts = attribute_emb[list(erased_item_att), :]

        all_u_att_emb = torch.sum(user_atts, dim=0, keepdim=True)
        all_i_att_emb = torch.sum(item_atts, dim=0, keepdim=True)


        u_es = concate_fusion(rec_model.user_embeddings.weight[[u for u in range(data.filtered_n_users)]], all_u_att_emb)
        i_es = concate_fusion(rec_model.item_embeddings.weight[[i for i in range(data.filtered_n_items)]], all_i_att_emb)

        # pre_loger, rec_loger, \
        ndcg_loger, hit_loger, long_tail, gini = [], [], [], []
        with torch.no_grad():
            ret = test_model(u_es, i_es, args_config.Ks, data)

        # rec_loger.append(ret["recall"])
        # pre_loger.append(ret["precision"])
        ndcg_loger.append(ret["ndcg"])
        hit_loger.append(ret["hit_ratio"])
        long_tail.append(ret["longtail_rate"])
        gini.append(ret["gini"])

        print_dict(ret)
        print('\n')

def erasure_evaluation_v2(rec_model, explanations, attribute_emb, data, erasure):
    """
    Each data point is generated by cumulatively removing
    top n features in the explanation lists provided by explanation methods.
    """

    att={}
    for row in data.candidate_attributes.node_id:
        att[row] = []
    for node, ats in zip(data.candidate_attributes.node_id,data.candidate_attributes.attribute_id):
        att[node].append(ats)

    u_att = dict(list(att.items())[0:data.n_users])
    i_att = dict(list(att.items())[data.n_users:])

    erasure_set = explanations.iloc[:, :erasure].values.flatten()

    for i in range(0, len(erasure_set), 1024):
        print('erasure number for this batch:', i)
        erasure = set(erasure_set[:i])
        erased_user_att = {}
        erased_item_att = {}
        for user, ats in u_att.items():
            erased_user_att[user] = set(ats)-erasure

        for item, ats in i_att.items():
            erased_item_att[item] = set(ats)-erasure

        # get attribute embedding.
        erased_user_emb = {}
        erased_item_emb = {}
        for user, ats in erased_user_att.items():
            concated_emb = torch.sum(attribute_emb[list(ats), :], dim=0, keepdim=True)
            erased_user_emb[user] = concated_emb
        for item, ats in erased_item_att.items():
            concated_emb = torch.sum(attribute_emb[list(ats), :], dim=0, keepdim=True)
            erased_item_emb[item] = concated_emb

        u_es_list = []
        for user in range(data.filtered_n_users):
            if user in erased_user_emb.keys():
                emb = concate_fusion(rec_model.user_embeddings.weight[user], erased_user_emb[user])
                u_es_list.append(emb)
            else:
                emb = concate_fusion(rec_model.user_embeddings.weight[user], torch.ones(1,128))
                u_es_list.append(emb)

        i_es_list = []
        for item in range(data.filtered_n_items):
            if item in erased_item_emb.keys():
                emb = concate_fusion(rec_model.item_embeddings.weight[item], erased_item_emb[item])
                i_es_list.append(emb)
            else:
                emb = concate_fusion(rec_model.item_embeddings.weight[item], torch.ones(1,128))
                i_es_list.append(emb)

        ndcg_loger, hit_loger, long_tail, gini = [], [], [], []
        with torch.no_grad():
            ret = test_model(torch.cat(u_es_list),torch.cat(i_es_list), args_config.Ks, data)

        ndcg_loger.append(ret["ndcg"])
        hit_loger.append(ret["hit_ratio"])
        long_tail.append(ret["head-tail_rate"])
        gini.append(ret["gini"])

        print_dict(ret)
        print('\n')


if __name__ == '__main__':
    args_config = parse_args()
    device = args_config.device
    print('Using dataset: {}'.format(args_config.dataset))

    if args_config.dataset == 'Douban Movie':
        data = DoubanMovie(args_config.dataset_path)
    elif args_config.dataset == 'Yelp':
        data = Yelp(args_config.dataset_path)
    elif args_config.dataset == 'LastFM':
        data = LastFM(args_config.dataset_path)
    else:
        raise NotImplementedError("{} not supported.".format(args_config.dataset))

    # # ---------------------------Load recommender embeddings--------------------------

    MF_train, MF_test = data.train_MF, data.test_MF

    recommender = BasePipeline(MF_train, test=MF_test, model=MFModule,
                               n_factors=args_config.n_factors, batch_size=args_config.MF_batch_size,
                               dropout_p=args_config.dropout_p,
                               lr=args_config.MF_lr, weight_decay=args_config.weight_decay,
                               optimizer=torch.optim.Adam, n_epochs=args_config.MF_epoch,
                               verbose=True, random_seed=2017)

    rec_model = recommender.model

    rec_model.load_state_dict(torch.load(os.path.join(args_config.model_out,
                                                      './recommender_{}_MFepoch_{}.pt'.format(args_config.dataset,
                                                                                              args_config.MF_epoch))))
    print('loaded recommender paras:', rec_model.eval())

    # # ---------------------------Load explanations and attribute embeddings--------------------------
    our_explanations = pd.read_csv(
        os.path.join(args_config.explanation_path, 'counterfactual_attribute_{}.csv'.format(args_config.dataset)))

    attribute_embeddings = torch.from_numpy(np.load(os.path.join(args_config.embedding_path,
                                                '{}_attribute_emb_{}_epoch_{}.npy'.format(args_config.dataset,
                                                                                          args_config.n_inp,
                                                                                          args_config.n_epoch))))


    # random_baseline(our_explanations, data, args_config)
    # pop_baseline(our_explanations, data, args_config)

    # load random explanations
    rand_explanations = pd.read_csv(
        os.path.join(args_config.explanation_path, 'random_attribute_{}.csv'.format(args_config.dataset)))

    user_pop =  pd.read_csv(
        os.path.join(args_config.explanation_path, 'user_pop_attribute_{}.csv'.format(args_config.dataset)))

    item_pop =  pd.read_csv(
        os.path.join(args_config.explanation_path, 'item_pop_attribute_{}.csv'.format(args_config.dataset)))

    erasure_evaluation_v2(rec_model, item_pop, attribute_embeddings, data, args_config.erasure)
